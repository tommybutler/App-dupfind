#!/usr/bin/env perl

use strict;
use warnings;

use 5.010;

BEGIN { STDERR->autoflush; STDOUT->autoflush; }

use Getopt::Long;
use Benchmark ':hireswallclock';

use lib 'lib';
use App::dupfind;
use App::dupfind::Threaded;

use subs 'say_stderr';

my $opts =
{
   bytes     => 1024 ** 3,           # 1 GB max read
   cachestop => ( 1024 ** 2 ) * 2,   # 2 MB file size limit on cache candidacy
   dir       => undef,
   format    => 'human',             # options are "human" or "robot"
   help      => undef,
   links     => 0,
   maxdepth  => 50,
   progress  => 0,
   prompt    => 0,
   ramcache  => ( 1024 ** 2 ) * 300, # 300 MB default cache size
   remove    => 0,
   threads   => 0,
   verbose   => 0,
   weed      => 1,
   wpass     => [ ],
   wpsize    => 32,
};

GetOptions
(
   'bytes|b=i'    => \$opts->{bytes},
   'cachestop=i'  => \$opts->{cachestop},
   'dir|d=s'      => \$opts->{dir},
   'format|f=s'   => \$opts->{format},
   'help|h|?'     => \$opts->{help},
   'links|l'      => \$opts->{links},
   'maxdepth|m=i' => \$opts->{maxdepth},
   'progress'     => \$opts->{progress},
   'prompt|p'     => \$opts->{prompt},
   'ramcache=i'   => \$opts->{ramcache},
   'remove|x'     => \$opts->{remove},
   'threads|t=i'  => \$opts->{threads},
   'verbose|v'    => \$opts->{verbose},
   'weedout|w=s'  => \$opts->{weed},
   'wpass=s'      =>  $opts->{wpass},
   'wpsize=i'     => \$opts->{wpsize},
) or exit usage();

exit usage() unless defined $opts->{dir};

$opts->{threads} //= 0;

$opts->{remove}    = 1 if $opts->{prompt};

$opts->{weed}      = 0 if $opts->{weed} =~ /^(n|no)$/i;

$opts->{weed}      = 1 if !! @{ $opts->{wpass} };

$opts->{wpass}     = [ 'first_middle_last' ]
   if $opts->{weed} && ! @{ $opts->{wpass} };

die 'Bogus cache parameters provided!  Just use the defaults buddy.'
   if $opts->{cachestop} < 0 || $opts->{ramcache} < 0;

die 'Inappropriate cachestop size; must be a small fraction of total ramcache.'
   if $opts->{ramcache} && $opts->{cachestop} > ( $opts->{ramcache} / 100 );

$opts->{cachesize} = int ( $opts->{ramcache} / $opts->{cachestop} );

my $fdf = $opts->{threads}
   ? App::dupfind::Threaded->new( opts => $opts )
   : App::dupfind->new( opts => $opts );

my $benchmarks =
{
   scanfs   => { },
   prune    => { },
   weed     => { },
   digest   => { },
   remove   => { },
   run      => { },
};

my ( $scan_count, $size_dup_count, $dup_count ) = run();

my ( $cache_hits, $cache_misses ) = $fdf->cache_stats;

say_stderr <<__SUMMARY__;
------------------------------
** THREADS...............$opts->{threads}
** RAM CACHE.............$opts->{ramcache} bytes
** CACHE HITS/MISSES.....$cache_hits/$cache_misses
** TOTAL FILES SCANNED...$scan_count
** TOTAL SAME SIZE.......$size_dup_count
** TOTAL ACTUAL DUPES....$dup_count
      -- TIMES --
** TREE SCAN TIME........$benchmarks->{scanfs}->{result}
** HARDLINK PRUNE TIME...$benchmarks->{prune}->{result}
** WEED-OUT TIME.........$benchmarks->{weed}->{result}
** CRYPTO-HASHING TIME...$benchmarks->{digest}->{result}
** DELETION TIME.........$benchmarks->{remove}->{result}
** TOTAL RUN TIME........$benchmarks->{run}->{result}
__SUMMARY__

exit;

sub run
{
   bench_this( run => 'start' );

   my ( $size_dups, $scan_count, $size_dup_count ) = scanfs();

   $size_dups = prune( $size_dups );

   $size_dups = weed( $size_dups ) if $opts->{weed};

   my $digest_dups = digest( $size_dups );

   undef $size_dups; # free up some RAM

   say_stderr '** DISPLAYING OUTPUT'; say_stderr '-' x 30;

   my $dup_count = $fdf->show_dups( $digest_dups );

   delete_dups( $digest_dups ) if $opts->{remove};

   bench_this( run => 'end' );

   calculate_bench_times();

   return $scan_count, $size_dup_count, $dup_count;
}

sub scanfs
{
   say_stderr '** SCANNING ALL FILES FOR SIZE DUPLICATES';

   bench_this( scanfs => 'start' );

   my ( $size_dups, $scan_count, $size_dup_count ) = $fdf->get_size_dups();

   bench_this( scanfs => 'end' );

   say '** NO DUPLICATES FOUND' and exit unless keys %$size_dups;

   return $size_dups, $scan_count, $size_dup_count;
}

sub prune
{
   my $size_dups = shift;

   say_stderr '** PRUNING HARD LINKS';

   bench_this( prune => 'start' );

   $size_dups = $fdf->toss_out_hardlinks( $size_dups );

   bench_this( prune => 'end' );

   say_stderr '** NO DUPLICATES FOUND' and exit unless keys %$size_dups;

   return $size_dups;
}

sub weed
{
   my $size_dups = shift;

   say_stderr '** WEEDING-OUT FILES THAT ARE OBVIOUSLY DIFFERENT';

   bench_this( weed => 'start' );

   $size_dups = $fdf->weed_dups( $size_dups );

   bench_this( weed => 'end' );

   say_stderr '** NO DUPLICATES FOUND' and exit unless keys %$size_dups;

   return $size_dups;
}

sub digest
{
   my $size_dups = shift;

   say_stderr '** CHECKSUMMING SIZE DUPLICATES';

   bench_this( digest => 'start' );

   my $digest_dups = $fdf->digest_dups( $size_dups );

   bench_this( digest => 'end' );

   say_stderr '** NO DUPLICATES FOUND' and exit unless keys %$digest_dups;

   return $digest_dups;
}

sub delete_dups
{
   my $digests = shift;

   bench_this( remove => 'start' );

   $fdf->delete_dups( $digests );

   bench_this( remove => 'end' );
}

sub bench_this
{
   my ( $mark, $start_end ) = @_;

   $benchmarks->{ $mark }->{ $start_end } = Benchmark->new();
}

sub calculate_bench_times
{
   for my $mark ( keys %$benchmarks )
   {
      next unless $benchmarks->{ $mark }->{start};

      $benchmarks->{ $mark }->{result} =
         timestr timediff
         (
            $benchmarks->{ $mark }->{end},
            $benchmarks->{ $mark }->{start}
         );
   }

   $benchmarks->{weed}->{result}   ||= 'did not weed';
   $benchmarks->{remove}->{result} ||= 'no deletions';
}

sub say_stderr { warn "$_\n" for @_ };

# This is just the help message:

sub usage { say_stderr <<'__USAGE__' }
USAGE:
   dupfind [ --options ] --dir ./path/to/search/

EXAMPLE:
   dupfind --threads 4 --format robot --maxdepth 100 --bytes 1099511627776 --dir /dedup

DESCRIPTION:
   finds duplicate files in a directory tree.  Options are explained
   in detail below.  Options marked with an asterisk (*) are not yet
   implemented and are planned for a future release

ARGUMENTS AND FLAGS:
   -b, --bytes    Maximum file size in bytes that you are willing to compare.
                  The current default maximum is 1 gigabyte.

                  Sizing guide:
                     1 kilobyte = 1024
                     1 megabyte = 1048576        or 1024 ** 2
                     1 gigabyte = 1073741824     or 1024 ** 3
                     1 terabyte = 1099511627776  or 1024 ** 4

   --cachestop    Integer indicating the maximum file size to put into the
                  cache of computed file digests.  Note that this is NOT the
                  max amount of RAM to consume for the cache. (see --ramcache)
                  Default value: 1 megabyte

   -d, --dir      Name of the directory you want to search for duplicates

   -f, --format   Specify either "human" or "robot".  Human-readable output is
                  generated for easy viewing by default.  If you want output
                  that is machine-parseable, specify "robot"

   -g, --progress Display a progress bar.  Why "-g"?  Because I ran out of "-p"s

*  -l, --links    Follow symlinks (by default it does not).  Because this
                  has some safety implications and is a complex matter,
                  it is not yet supported.  Sorry, check back later.

   -m, --maxdepth The maximum directory depth to which the comparison scan will
                  recurse.  Note that this does not mean the total number of
                  directories to scan.

   -p, --prompt   Interactively prompt user to delete detected duplicates

   --ramcache     Integer indicating the number of bytes of RAM to consume
                  for the cache of computed file digests.  Note that dupfind
                  will still use a substantial amount of memory for other
                  internal purposes that don't have to do with the cache.
                  Default: 100 megabytes.  Set to 0 to disable ram cache.

   -x, --remove   CAUTION: Delete WITHOUT PROMPTING all but the first copy
                  if duplicate files are found.  This will leave you with no
                  duplicate files when execution is finished.

   -t, --threads  Number of threads to use for file comparisons.  Defaults
                  to 10, but lower numbers will do better on systems with
                  fewer cores.  You'll usually get best performance using a
                  number of threads equal to the number of logical processors
                  on your system, plus 1.

   -v, --verbose  Gives you a progress bar (like --progress) and some extra,
                  helpful output if you need more detail about file dupes
                  detected by dupfind.

   -w, --weedout  Either yes or no.  (Default yes).  Tries to avoid unnecessary
                  file hashing by weeding out potential duplicates with a
                  simple, quick comparison of the last 1024 bytes of data in
                  same-size files.  This typically produces very significant
                  performance gains, especially with large numbers of files.

   --wpass        One or more of the following "weeding" pass filters can be
                  specified.  Weed-out passes reduce the amount of
                  cryptographic digest calculations that must happen by
                  weeding out potential-duplicates:

                              first   (checks first few bytes of each file)
                             middle   (checks the center-most single byte)
                               last   (checks the last few bytes of files)
                        middle_last   (checks middle byte and last few bytes)
                  first_middle_last   (first bytes, middle byte, last bytes)

                  The default is to only run the "first_middle_last" weed-out
                  filter pass, which usually yields the best results in terms
                  of speed.

                  Weed-out filters are executed in the order you specify.

                  Example usage: dupfind --wpass first --wpass last [...]

   --wpsize       Integer indicating the number of bytes to read per file
                  during a weed-out pass.  Default: 32.  If your weed-out
                  pass type reads a file in two or more places, this value
                  will be used for each read except "middle" reads, which
                  are always 1 byte only.

__USAGE__
