#!/usr/bin/env perl

use strict;
use warnings;

use 5.010;

BEGIN { $|++ }

use Getopt::Long;
use Benchmark ':hireswallclock';

use lib 'lib';
use File::DupFind;
use File::DupFind::Threaded;

my $opts =
{
   dir      => undef,
   help     => undef,
   bytes    => 1024 ** 3, # 1 GB max read
   maxdepth => 10,
   prompt   => 0,
   remove   => 0,
   links    => 0,
   threads  => 0,
   format   => 'human', # formatting options are either "human" or "robot"
   weed     => 1,
   qsize    => 30,
};

GetOptions
(
   'dir|d=s'      => \$opts->{dir},
   'bytes|b=i'    => \$opts->{bytes},
   'maxdepth|m=i' => \$opts->{maxdepth},
   'links|l'      => \$opts->{links},
   'prompt|p'     => \$opts->{prompt},
   'remove|x'     => \$opts->{remove},
   'help|h|?'     => \$opts->{help},
   'threads|t=i'  => \$opts->{threads},
   'format|f=s'   => \$opts->{format},
   'weed|w=s'     => \$opts->{weed},
   'qsize|q=i'    => \$opts->{qsize},
) or die usage();

die usage() unless defined $opts->{dir};

$opts->{remove}++  if $opts->{prompt};
$opts->{threads} //= 0;
$opts->{weed}      = 0 if $opts->{weed} =~ /^(n|no)$/i;

my $fdf = $opts->{threads}
   ? File::DupFind::Threaded->new( opts => $opts )
   : File::DupFind->new( opts => $opts );

my $benchmarks =
{
   scanfs   => { },
   prune    => { },
   weed     => { },
   digest   => { },
   remove   => { },
   run      => { },
};

my ( $scan_count, $size_dup_count, $dup_count ) = run();

say <<__SUMMARY__;
------------------------------
** THREADS...............$opts->{threads}
** TOTAL FILES SCANNED...$scan_count
** TOTAL SAME SIZE.......$size_dup_count
** TOTAL ACTUAL DUPES....$dup_count
      -- TIMES --
** TREE SCAN TIME........$benchmarks->{scanfs}->{result}
** HARDLINK PRUNE TIME...$benchmarks->{prune}->{result}
** WEED-OUT TIME.........$benchmarks->{weed}->{result}
** CRYPTO-HASHING TIME...$benchmarks->{digest}->{result}
** DELETION TIME.........$benchmarks->{remove}->{result}
** TOTAL RUN TIME........$benchmarks->{run}->{result}
__SUMMARY__

exit;


sub run
{
   bench_this( run => 'start' );

   my ( $size_dups, $scan_count, $size_dup_count ) = scanfs();

   $size_dups = prune( $size_dups );

   $size_dups = weed( $size_dups );

   my $digest_dups = digest( $size_dups );

   undef $size_dups; # free up some RAM

   say '** DISPLAYING OUTPUT'; say '-' x 30;

   my $dup_count = $fdf->show_dups( $digest_dups );

   delete_dups( $digest_dups ) if $opts->{remove};

   bench_this( run => 'end' );

   calculate_bench_times();

   return $scan_count, $size_dup_count, $dup_count;
}

sub scanfs
{
   say '** SCANNING ALL FILES FOR SIZE DUPLICATES';

   bench_this( scanfs => 'start' );

   my ( $size_dups, $scan_count, $size_dup_count ) = $fdf->get_size_dups();

   bench_this( scanfs => 'end' );

   say '** NO DUPLICATES FOUND' and exit unless keys %$size_dups;

   return $size_dups, $scan_count, $size_dup_count;
}

sub prune
{
   my $size_dups = shift;

   say '** PRUNING HARD LINKS';

   bench_this( prune => 'start' );

   $size_dups = $fdf->toss_out_hardlinks( $size_dups );

   bench_this( prune => 'end' );

   say '** NO DUPLICATES FOUND' and exit unless keys %$size_dups;

   return $size_dups;
}

sub weed
{
   my $size_dups = shift;

   say '** WEEDING-OUT FILES THAT ARE OBVIOUSLY DIFFERENT';

   bench_this( weed => 'start' );

   $size_dups = $fdf->weed_dups( $size_dups );

   bench_this( weed => 'end' );

   say '** NO DUPLICATES FOUND' and exit unless keys %$size_dups;

   return $size_dups;
}

sub digest
{
   my $size_dups = shift;

   say '** CHECKSUMMING SIZE DUPLICATES';

   bench_this( digest => 'start' );

   my $digest_dups = $fdf->get_dup_digests( $size_dups );

   bench_this( digest => 'end' );

   say '** NO DUPLICATES FOUND' and exit unless keys %$digest_dups;

   return $digest_dups;
}

sub delete_dups
{
   my $digests = shift;

   bench_this( remove => 'start' );

   $fdf->delete_dups( $digests );

   bench_this( remove => 'end' );
}

sub bench_this
{
   my ( $mark, $start_end ) = @_;

   $benchmarks->{ $mark }->{ $start_end } = Benchmark->new();
}

sub calculate_bench_times
{
   for my $mark ( keys %$benchmarks )
   {
      next unless $benchmarks->{ $mark }->{start};

      $benchmarks->{ $mark }->{result} =
         timestr timediff
         (
            $benchmarks->{ $mark }->{end},
            $benchmarks->{ $mark }->{start}
         );
   }

   $benchmarks->{weed}->{result}   ||= 'did not weed';
   $benchmarks->{remove}->{result} ||= 'no deletions';
}

# This is just the help message:

sub usage { <<'__USAGE__' }
USAGE:
   dupfind [ --options ] --dir ./path/to/search/

EXAMPLE:
   dupfind --threads 4 --format robot --maxdepth 100 --bytes 1099511627776 --dir /dedup

DESCRIPTION:
   finds duplicate files in a directory tree.  Options are explained
   in detail below.  Options marked with an asterisk (*) are not yet
   implemented and are planned for a future release

ARGUMENTS AND FLAGS:
   -b, --bytes    Maximum size in bytes that you are willing to compare.
                  The current default maximum is 1 gigabyte.

                  Sizing guide:
                     1 kilobyte = 1024
                     1 megabyte = 1048576        or 1024 ** 2
                     1 gigabyte = 1073741824     or 1024 ** 3
                     1 terabyte = 1099511627776  or 1024 ** 4

   -d, --dir      Name of the directory you want to search for duplicates

   -f, --format   Specify either "human" or "robot".  Human-readable output
                  is generated for easy viewing by default.  If you want output
                  that is machine-parseable, specify "robot"

*  -l, --links    Follow symlinks (by default it does not).  Because this
                  has some safety implications and is a complex matter,
                  it is not yet supported.  Sorry, check back later.

   -m, --maxdepth The maximum directory depth to which the comparison
                  scan will recurse.  Note that this does not mean the
                  total number of directories to scan.

   -p, --prompt   Interactively prompt user to delete detected duplicates

   -q, --qsize    Number of files each thread should process at a time. The
                  the default is 30.

   -x, --remove   CAUTION: Delete WITHOUT PROMPTING all but the first copy
                  if duplicate files are found.  This will leave you with no
                  duplicate files when execution is finished.

   -t, --threads  Number of threads to use for file comparisons.  Defaults
                  to 10, but lower numbers will do better on systems with
                  fewer cores.  You'll usually get best performance using a
                  number of threads equal to the number of logical processors
                  on your system, plus 1.

   -w, --weed     Either yes or no.  (Default yes).  Tries to avoid unnecessary
                  file hashing by weeding out potential duplicates with a
                  simple, quick comparison of the last 1024 bytes of data in
                  same-size files.  This typically produces very significant
                  performance gains, especially with large numbers of files.

__USAGE__
